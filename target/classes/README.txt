实验目标：

主要针对的是需要长时间存储的场景，因此编码时间和解码时间相对于存储时间是可以忽略不计的
所以，灾害事件考虑发生在编码完成之后，解码收集之前，随机选取网络中的节点进行破坏，观察最后的数据恢复情况

1.减少大规模网络中节点存储数据的能源消耗（分区）
2.针对收集器无法进入勘测区域实地收集数据的情况，采取分层的方式将监测区域内的数据通过层次的方式传递到最外层的节点上，
收集器通过访问最外层节点从而达到收集全部数据的目的。
3.分层的话可以在收集阶段尽早的将1度包发送到相对安全的最外层节点上进行存储，保证解码能够顺利进行；如若不然，
倘若数据收集的前中期大部分1度包遭到破坏，并且由于LT的延迟效应以及1度包所占比例较小，可能会导致解码无法正常进行；

并且通过轮训邻居的方式传递节点不仅是为了分摊资源的消耗，也更是为了防止某些度的包集中存储于某个外层节点而导致集中损坏无法继续解码。
考虑将1度节点均匀分摊到各个外层节点而没有专门集中存储在1个最外层节点上是为了防止极端情况下存储1度包的最外层节点遭到破坏会使得数据恢复失败


在场景中铺设节点阶段，有2类节点，感知节点和编码节点，各自均匀分布在整个网络环境中

编码阶段，感知节点向网络中一共发送b个数据包，数据包携带的就是感知节点的id信息，即sensorId，而编码就是将感知节点的id信息进行编码存储
最后收集阶段就是看是否能够收集完全部的感知节点的id信息，因为是均匀铺设的，即能不能收集到所有id为奇数的id信息

节点继承链关系：                  -> 分层的LT节点
              普通节点 -> LT节点 -> 分区分层的LT节点
                               -> 分区的LT节点

奇数的是感知节点，偶数的是编码节点

由于转移表Pij中M是图中最大节点的度，因此想要得到M的值必须等编码阶段结束之后才可以，可以通过编码完毕后遍历所有节点的邻居集合的数量，取最大的就是M
但记住，给每个节点添加转移表的时候要先探测一下邻居，然后每次转移的时候都要更新一下转移表

分区节点:铺设的时候直接进行分区，1,2,3,4这样进行重复循环分区

编码阶段：对于所有的节点，暂时设定的是什么数据包都接受，暂时不做区别

//TODO 对于转移表的生成，总概率可能会大于1，遇到这种情况的实验应该放弃，重新进行实验

//TODO 转移表那里是不是有问题， 如果是按照论文那种的那样设定转移表，会使得自转发的概率非常大，因此可能需要调整通信半径的大小
//TODO 目前想到的解决方案是将M取为当前的节点的邻居中最大的度，并且对于一个相同的数据包，用一个节点不会接受2次以上
//TODO 即对于随机游走，如果最后落的节点已经有过该数据包的数据了，那么继续转发;如果分区了，最后的节点如果不是本区的也会继续游走

//TODO 由于实际度与理想度不一致导致的偏差会严重影响实验结果，因此对于每个节点，做一定的限制，如果度已经达到要求了，那么不再接受数据包，而只是转发数据包
//TODO 然后在通过加大随机游走的步长，使实际度无线接近理想度。这里之所以会卡住可能是因为概率转移表的问题，某些点的接收概率太高了。
//TODO 因此考虑将M设为当前节点的邻居数，加大各个节点的接受率，使得随机游走更加有效

当节点个数过多的时候，如果节点通信半径还很小的话会导致节点的邻居数量很少，这样会导致计算转移表的时候有非常大的概率会自转发

关于灾难发生的时机需要确定，计划是在所有编码完成后开始进行灾难突发监测


//TODO 分区的实验：感知节点产生b份数据时b的大小是要小于未分区时的b

需要有的图：
1.理想的度分布概率图，真实的度分布概率图
2.收到的编码数据包数量和恢复源数据的函数图
3.随机步数和解码率
4.分区和没有分区的编码复杂度与编码数量的对比图
5.分层和不分层的LT对比


// 改进实验：
对于随机游走进行编码的阶段，如果节点还没有到达要求的度，那么只要是一个自己列表中没有的数据包就直接进行编码；如果是自己列表中已经有的，那么直接进行转发。
如果节点达到了要求的度，那么不管是不是自己的都直接进行转发。

对于步数耗尽的数据包，如果数据包最后的落下的节点列表中已经有了该数据包的信息，那么该数据包直接消亡；否则，该节点接收该数据包进行编码。

总体来说：c相对大一点数据恢复情况会好一点，因为相对尖峰值比较提前，即高度包所占比例较少；分区数量的话维持在3左右效果最好

c如果小的话分层效果相对好，分区效果相对差；
c如果大的话分区效果相对好，分层效果相对差；
比较难找到一个折中点，使得同时分层分区取得较好的效果

关于分区的数量，通过计算编码所需节点数量得到结论：
如果C比较小，尖峰值节点的度要求比较大，分区后的随机游走可能很难满足各区能够达到理想的度要求，因此如果C比较小，综合来看，3分区的数量整体效果比较好，经分析可能是因为
分区后，总体编码数量随分区数量的增加而增加，但是本模型下总编码节点数量是固定的，并且前期C较小时尖峰值节点的度较高，分区后节点较难保证各区内都能达到理想的度要求；
但是随着C的慢慢变大，达到0.2左右，尖峰值节点逐渐下降，分区后各区的尖峰值节点也随之下降，随机游走较想要达到理想的度要求变得比较容易，再加之3分区下的理想总编码节点数量并
没有太多显著的提升，因此整体均衡效果最好。
并且当C增大到0.23时数据恢复效果已经要明显好于普通LT，此时此模型下的普通LT已经不能恢复所有的源数据，因为理想的编码节点总数量已经不能达到满足。

然后又针对3分区下的编码节点按照随机游走步长从500到2000进行实验，发现并没有太大的变化，因此500的随机游走步长已经可以满足要求

分层结构下破坏性试验较差的原因可能是某些关键性节点周围的邻居节点遭到破坏，无法将该节点的数据通过邻居发送到最外层的节点上进行存储，当然，这也与我们的网络结构是有关系的
，倘若是稠密网络，可能会稍微好一点，但是若是稀疏网络，可能会相对差一点（节点的通信半径）。




// 3分区之后分区的和普通LT在0.24左右解码率最高，因此就0.24比较二者的数据恢复情况，再比较二者在灾难下的数据恢复情况

// 在做一下3分区的所需编码符号总数和平均编码度的二者的对比图

//TODO 分区编码是不现实的，因为随着c的变大，本身LT的所需编码符号数量和平均编码度是会变大的，因此与c=0.01时的LT Codes相比是没有任何优势的
//TODO 考虑还是使用二进制指数+鲁棒骨子分布的形式改进度分布函数来减少高度包所占比例，观察恢复情况